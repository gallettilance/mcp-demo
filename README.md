# Llama Stack + MCP demo

## Run the MCP server

MCP server that can do math to extend an LLM's capability.

1. Install stuff `pip install -e .`
2. Run the MCP server with `mcp-lance --verbose`

## Run the llama stack

1. `export OPENAI_API_KEY="..."`
2. Run `llama stack run run.yml`

